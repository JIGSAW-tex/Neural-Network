{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adba3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfdb85-38d7-4e15-8180-8da724a95902",
   "metadata": {},
   "source": [
    "Coding a single neuron which is basically a linear regresssion having Weinghts and biases   \n",
    "Also it only has  one neuron without activation funtion   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb495ef6-5ec0-4542-993c-41bfcd7b6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.array([1.0 , 2.0 , 3.0 , 2.5 ])\n",
    "w = np.array([0.2 , 0.8 , 0.5 , 1.0])\n",
    "b = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6a73a1-1c1d-40f6-ac1a-3b8a79e9ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#neuron\n",
    "n = 0 \n",
    "for i in range(w.shape[0]): #w.shape[0] defines the no. of rows\n",
    "    n += inp[i]*w[i] \n",
    "n+=b \n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b61a1f7-7196-4a9a-a65c-e32585d97ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.dot(inp, w) + b\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d585b51a-a7e5-4802-8d82-d0000579b14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2,  0.7, -0.6,  1.5,  0.9],\n",
       "       [-0.5,  0.3,  0.8, -0.4,  1.1],\n",
       "       [ 1. , -1.2,  0.4,  2.3, -0.7]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for a layer with three neuron \n",
    "x = np.array([[1.2, 0.5, -0.7, 3.4, 2.1]])\n",
    "W = np.array([[ 0.2, -0.5,  1. ],\n",
    "       [ 0.7,  0.3, -1.2],\n",
    "       [-0.6,  0.8,  0.4],\n",
    "       [ 1.5, -0.4,  2.3],\n",
    "       [ 0.9,  1.1, -0.7]\n",
    "])\n",
    "b = np.array([0.5, -0.2, 1.0])\n",
    "W  = W.T\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bda956-2607-4710-a1a0-61673a91c1de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,5) and (3,5) not aligned: 5 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m      2\u001b[0m f\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,5) and (3,5) not aligned: 5 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "f = np.dot(x , W) + b\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ab6d7-88a3-4649-a8fd-f4023468e18a",
   "metadata": {},
   "source": [
    " Neurons  hai isiliye weights itne  hai like for each feature there is a wieght and for each neuron there are  feature with  different weights   \n",
    " So in the above case there are 5 features and 3 neurons hence the no. of weights and bias are 15 and 3 respectively   \n",
    " also the no. of rows represent no. of samples or batches which in this case is just one   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5de4031-045a-4c6e-8641-fbd3776a9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for passing a neuron from one neuron to another \n",
    "x= np.array([[1.2, 0.5, -0.7, 3.4, 2.1\n",
    "             ]]) # 5 features passed through first neuron\n",
    "w1 = np.array([[ 0.2,  0.7, -0.6,  1.5,  0.9], # weights of 3 neurons in five features  1st layer\n",
    "       [-0.5,  0.3,  0.8, -0.4,  1.1], \n",
    "       [ 1. , -1.2,  0.4,  2.3, -0.7]]) \n",
    "b1 = np.array([0.5, -0.2, 1.0]) # biases for three neurons of 1st layer \n",
    "\n",
    "w2 = np.array([[0.1, -0.14, 0.5],  #Weights of threee freatures and there neurons \n",
    "        [-0.5, 0.12, -0.33],\n",
    "        [-0.44, 0.73, -0.13]])\n",
    "b2 = np.array([2, 3, 0.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef963bc-29de-4547-acf6-1f06cb614b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.dot(x , w1.T) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbd094dc-6670-43b3-9597-2d2c0d182139",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = np.dot(n1 , w2) + b2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870e253e-2e67-4dc8-bbab-eeb0380f05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = np.array([5 , 3 , 1 ])  # one neuron in the third layer \n",
    "b3 = np.array([1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6203eff-07ee-4080-80b4-a0d64839f6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.9984])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n3 = n2 @ w3.T + b3\n",
    "n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "503b8f02-0cdf-40b4-b3b9-31a6c865c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suhail nigga\n",
      "Nigga\n",
      "Representation\n"
     ]
    }
   ],
   "source": [
    "class blue:\n",
    "    def __init__(self , nig : str): # slef its like constructor in java \n",
    "        self.name = nig  # its like the global variable :) so basic shit that i understood now\n",
    "    def methods(self  , isit  : bool): \n",
    "        if isit == True :\n",
    "            print(\"Suhail is a nigga\")\n",
    "        else :\n",
    "            print(\"Still suhail is a nigga\")\n",
    "    def __add__(self , other) :\n",
    "        name = other.name\n",
    "        print(self.name , name)\n",
    "    def __str__(self) -> str:\n",
    "        return \"Nigga\"\n",
    "    def __repr__(self) -> str :\n",
    "        return \"Representation\"\n",
    "hey = blue(\"Suhail\")\n",
    "noo = blue(\"nigga\") # agr self nhi diya toh khud se variable define karna padega penchoo\n",
    "(hey + noo)\n",
    "print(hey )\n",
    "print(repr(hey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "887588d3-f7c5-4893-8618-24037ba7cd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_output = np.array([0.1 ,0.1 , 0.8]) # bro its the probability of one neuron with three distinct probability of happenning  also its for one batch \n",
    "# categorical cross entropy loss \n",
    "# we labeled them as one  cross entropy  also it avg across samples and not neurons \n",
    "#assuming its one hot encoded wht we can do is \n",
    "cross_entropy = - np.log(np.max(softmax_output))\n",
    "cross_entropy.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3394e7e5-2c5c-411a-a4b1-480c44dd3e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1. , 0.5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in the case of having value less than 0.33 actualllly show that the hoghest propability is not selected having loss greter than 1 \n",
    "softmax_output = np.array([[0.1 ,0.1 , 0.8],\n",
    "                         [1 , 0 , 0] ,\n",
    "                          [0.1 , 0.5 , 0.4]])\n",
    "class_targets = [2 , 0 , 1 ]\n",
    "output = softmax_output[np.arange(softmax_output.shape[0]) , class_targets]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02c8f2b5-fdce-40e5-9b43-279733240861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1. , 0.5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(softmax_output , axis = 1 ) # lol got the same output in both but the issue is that the above one is genuine bcz of the class target sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "557cfe3d-5b09-4083-8287-62445d5b5b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3054302439580517"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to claculate the cross entropy for all three as well as take the avg :)\n",
    "cross_entropy = -np.log(output)\n",
    "cross_entropy = np.mean(cross_entropy)\n",
    "cross_entropy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0a68a08-7821-4fd8-b65d-597a5281bc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8      , 0.9999999, 0.5      ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another method without labeling them as 0,1 and 2 is \n",
    "#  = [2 , 0 , 1 ] this can be one hot encodeded  as \n",
    "y_target = np.array([[0 , 0 ,1] ,\n",
    "                   [1 , 0 , 0 ],\n",
    "                   [0 , 1 , 0 ]])\n",
    "softmax_output = np.array([[0.1 ,0.1 , 0.8],\n",
    "                         [1 , 0 , 0] ,\n",
    "                          [0.1 , 0.5 , 0.4]])\n",
    "softmax_output = np.clip(softmax_output , 1e-7 , 1- 1e-7 ) # ayoooo les gooooooo\n",
    "# actually we perform element wise multiplication thuss \n",
    "output = np.sum(y_target*softmax_output  , axis = 1 )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "171c7a4d-6c0b-4af2-8844-c9c691312633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3054302439580517"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = -np.log(output)\n",
    "cross_entropy = np.mean(cross_entropy)\n",
    "cross_entropy.tolist()  # see same as that shit mannnnnnnnn crazyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49625992-79a3-4b02-af83-2cfcb5dca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also we clip them to close to zero and close to one bcz exacts of them will lead log to infinity or gradient calculation will be fucked up \n",
    "# so the clipping is done using np.clip and  its done btw 10^-7 and 1- 10^-7 to avoid exact values :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
